{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "# import torch.nn.functional as F\n",
    "import pytorch_lightning\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Vision Transformer](vit.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, embedding_dim, d_model=256, nhead=4, num_layers=2):\n",
    "        super(Encoder, self).__init__()\n",
    "        \"encoder layer is single encoder block that is shown in above pic  (right)\"\n",
    "        self.encoder = nn.TransformerEncoder(\n",
    "            encoder_layer=nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead),\n",
    "            num_layers=num_layers,\n",
    "            norm = nn.LayerNorm(normalized_shape=embedding_dim)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        out = self.encoder(x)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VisionTransformer(nn.Module):\n",
    "    \"\"\"\n",
    "        transformer module to encoder the image patches\n",
    "        output of this module will be flatten encoded patches\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, image_size, channels, patch_size, stride, embedding_dim, nhead, num_layers, num_classes, fc_dim=256):\n",
    "        super(VisionTransformer, self).__init__()\n",
    "        \n",
    "        self.patch_size = patch_size\n",
    "        self.num_patches = (image_size // patch_size) ** 2\n",
    "        self.patch_dim = channels * patch_size ** 2\n",
    "        self.stride = stride\n",
    "\n",
    "        # patch_pos embedding and patch projection layer\n",
    "        self.pos_embedding = nn.Parameter(torch.randn(1, self.num_patches + 1, embedding_dim))\n",
    "        self.patch_projection = nn.Linear(in_features=self.patch_dim, out_features=embedding_dim, bias=False)\n",
    "        self.cls_token = nn.Parameter(torch.randn(1, 1, embedding_dim)) \n",
    "        \n",
    "        # transformer module to encoder the image patches output of this module will be flatten encoded patches\n",
    "        self.transformer = nn.TransformerEncoder(\n",
    "            encoder_layer=nn.TransformerEncoderLayer(d_model=embedding_dim, nhead=nhead),\n",
    "            num_layers=num_layers,\n",
    "            norm=nn.LayerNorm(normalized_shape=embedding_dim)\n",
    "        )\n",
    "        \n",
    "        # to take cls token\n",
    "        self.to_cls_token = nn.Identity()\n",
    "        # classifier or mlp head to classify the data\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(in_features=embedding_dim, out_features=fc_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=fc_dim, out_features=num_classes)\n",
    "        )\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x.shape = [batch, w, h, channel]\n",
    "        # patchifyt the image\n",
    "        batch_size = x.shape[0]\n",
    "        \n",
    "        x = self.patchify(x, self.patch_size, self.patch_size)\n",
    "        x = self.patch_projection(x)\n",
    "        \n",
    "        # concat cls token into projected patch\n",
    "        cls_token = self.cls_token.expand(batch_size, -1, -1) \n",
    "        x = torch.cat((cls_token, x), dim=1)\n",
    "        # add positional embedding + projected patches \n",
    "        x = x + self.pos_embedding\n",
    "        \n",
    "        # encoded the input and take the cls token and then feed it to mlp\n",
    "        x = self.transformer(x)\n",
    "        x = self.to_cls_token(x[:, 0])        \n",
    "        outputs = self.fc(x)\n",
    "        return outputs\n",
    "        \n",
    "        \n",
    "    def patchify(self, images, patch_size, stride):\n",
    "        # get all image windows of size (patch_size, patch_size) and stride (stride, stride)\n",
    "        patches = images.unfold(2, patch_size, stride).unfold(3, patch_size, stride)\n",
    "        patches = patches.permute(0, 2, 3, 4, 5, 1).contiguous()\n",
    "        # patches.shape -> [batch, ... .... ... ..., ]\n",
    "        # the size of flatten vector\n",
    "        bs, pr, pc, h, w, ch = patches.shape[0], patches.shape[1], patches.shape[2], patches.shape[3], patches.shape[4], patches.shape[5]\n",
    "        # bs->batch_size, rp->patches_row, pc->patches_col, h->patch_height, w->patch_width, w->patch_widht, ch->channels\n",
    "\n",
    "        # dissolve it \n",
    "        patches = patches.view(bs, pr*pc, h*w*ch)\n",
    "\n",
    "        return patches\n",
    "        \n",
    "    \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "vit = VisionTransformer(\n",
    "    image_size=256,\n",
    "    channels=3,\n",
    "    patch_size=16,\n",
    "    stride=16,\n",
    "    embedding_dim=512,\n",
    "    nhead=8,\n",
    "    num_layers=2,\n",
    "    num_classes=100\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.rand(10, 3, 256, 256)\n",
    "outputs = vit(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 100])"
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.6 64-bit ('base': conda)",
   "language": "python",
   "name": "python37664bitbaseconda1b4d65181bfe435290e55078ed6e0090"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
