{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "# import torch.nn.functional as F\n",
    "import pytorch_lightning\n",
    "import wandb\n",
    "import os\n",
    "import torchvision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Vision Transformer](vit.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, embedding_dim, d_model=256, nhead=4, num_layers=2):\n",
    "        super(Encoder, self).__init__()\n",
    "        \"encoder layer is single encoder block that is shown in above pic  (right)\"\n",
    "        self.encoder = nn.TransformerEncoder(\n",
    "            encoder_layer=nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead),\n",
    "            num_layers=num_layers,\n",
    "            norm = nn.LayerNorm(normalized_shape=embedding_dim)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        out = self.encoder(x)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VisionTransformer(nn.Module):\n",
    "    \"\"\"\n",
    "        transformer module to encoder the image patches\n",
    "        output of this module will be flatten encoded patches\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, image_size, channels, patch_size, stride, embedding_dim, nhead, num_layers, num_classes, fc_dim=256):\n",
    "        super(VisionTransformer, self).__init__()\n",
    "        \n",
    "        self.patch_size = patch_size\n",
    "        self.num_patches = (image_size // patch_size) ** 2\n",
    "        self.patch_dim = channels * patch_size ** 2\n",
    "        self.stride = stride\n",
    "\n",
    "        # patch_pos embedding and patch projection layer\n",
    "        self.pos_embedding = nn.Parameter(torch.randn(1, self.num_patches + 1, embedding_dim))\n",
    "        self.patch_projection = nn.Linear(in_features=self.patch_dim, out_features=embedding_dim, bias=False)\n",
    "        self.cls_token = nn.Parameter(torch.randn(1, 1, embedding_dim)) \n",
    "        \n",
    "        # transformer module to encoder the image patches output of this module will be flatten encoded patches\n",
    "        self.transformer = nn.TransformerEncoder(\n",
    "            encoder_layer=nn.TransformerEncoderLayer(d_model=embedding_dim, nhead=nhead),\n",
    "            num_layers=num_layers,\n",
    "            norm=nn.LayerNorm(normalized_shape=embedding_dim)\n",
    "        )\n",
    "        \n",
    "        # to take cls token\n",
    "        self.to_cls_token = nn.Identity()\n",
    "        # classifier or mlp head to classify the data\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(in_features=embedding_dim, out_features=fc_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=fc_dim, out_features=num_classes)\n",
    "        )\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x.shape = [batch, w, h, channel]\n",
    "        # patchifyt the image\n",
    "        batch_size = x.shape[0]\n",
    "        \n",
    "        x = self.patchify(x, self.patch_size, self.patch_size)\n",
    "        x = self.patch_projection(x)\n",
    "        \n",
    "        # concat cls token into projected patch\n",
    "        cls_token = self.cls_token.expand(batch_size, -1, -1) \n",
    "        x = torch.cat((cls_token, x), dim=1)\n",
    "        # add positional embedding + projected patches \n",
    "        x = x + self.pos_embedding\n",
    "        \n",
    "        # encoded the input and take the cls token and then feed it to mlp\n",
    "        x = self.transformer(x)\n",
    "        x = self.to_cls_token(x[:, 0])        \n",
    "        outputs = self.fc(x)\n",
    "        return outputs\n",
    "        \n",
    "        \n",
    "    def patchify(self, images, patch_size, stride):\n",
    "        # get all image windows of size (patch_size, patch_size) and stride (stride, stride)\n",
    "        patches = images.unfold(2, patch_size, stride).unfold(3, patch_size, stride)\n",
    "        patches = patches.permute(0, 2, 3, 4, 5, 1).contiguous()\n",
    "        # patches.shape -> [batch, ... .... ... ..., ]\n",
    "        # the size of flatten vector\n",
    "        bs, pr, pc, h, w, ch = patches.shape[0], patches.shape[1], patches.shape[2], patches.shape[3], patches.shape[4], patches.shape[5]\n",
    "        # bs->batch_size, rp->patches_row, pc->patches_col, h->patch_height, w->patch_width, w->patch_widht, ch->channels\n",
    "\n",
    "        # dissolve it \n",
    "        patches = patches.view(bs, pr*pc, h*w*ch)\n",
    "\n",
    "        return patches\n",
    "        \n",
    "    \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "vit = VisionTransformer(\n",
    "    image_size=256,\n",
    "    channels=3,\n",
    "    patch_size=16,\n",
    "    stride=16,\n",
    "    embedding_dim=512,\n",
    "    nhead=8,\n",
    "    num_layers=2,\n",
    "    num_classes=100\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.rand(10, 3, 256, 256)\n",
    "outputs = vit(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 100])"
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training with PyTorch-Lightning and WandB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "import pytorch_lightning.loggers as loggers\n",
    "import pytorch_lightning.metrics as metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {\n",
    "    # model args\n",
    "    \"image_size\":28,\n",
    "    \"channels\":1,\n",
    "    \"patch_size\":7,\n",
    "    \"stride\":7,\n",
    "    \"embedding_dim\":512,\n",
    "    \"nhead\":8,\n",
    "    \"num_layers\":4,\n",
    "    \"num_classes\":10,\n",
    "    \"fc_dim\":256,\n",
    "    # data\n",
    "    \"batch_size\":32,\n",
    "    \"num_workers\":4,\n",
    "    \"transforms\":torchvision.transforms.Compose([torchvision.transforms.ToTensor(), torchvision.transforms.Normalize((0.1307,), (0.3081,))]),\n",
    "    #training\n",
    "    \"lr\":0.01,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nn.CrossEntropyLoss?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [],
   "source": [
    "class  LightningViT(pl.LightningModule):\n",
    "    \n",
    "    def __init__(self, args):\n",
    "        super(LightningViT, self).__init__()\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        self.accuracy = metrics.Accuracy(num_classes=args[\"num_classes\"])\n",
    "        self.args = args\n",
    "        \n",
    "        self.model = VisionTransformer(\n",
    "            image_size=args[\"image_size\"],\n",
    "            channels=args[\"channels\"],\n",
    "            patch_size=args[\"patch_size\"],\n",
    "            stride=args[\"stride\"],\n",
    "            embedding_dim=args[\"embedding_dim\"],\n",
    "            nhead=args[\"nhead\"],\n",
    "            num_layers=args[\"num_layers\"],\n",
    "            num_classes=args[\"num_classes\"],\n",
    "            fc_dim=args[\"fc_dim\"]\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        outputs = self.model(x)\n",
    "        return outputs\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(params=self.parameters(), lr=self.args[\"lr\"]),\n",
    "    \n",
    "    def train_dataloader(self):\n",
    "        dataset = torchvision.datasets.MNIST(root=os.getcwd(), train=True, download=True, transform=self.args[\"transforms\"])\n",
    "        train_loader = DataLoader(dataset=dataset, batch_size=self.args[\"batch_size\"], shuffle=True, num_workers=self.args[\"num_workers\"])\n",
    "        return train_loader\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        outputs = self(x)\n",
    "        loss = self.criterion(outputs, y)\n",
    "        acc = self.accuracy(outputs.argmax(dim=1), y)\n",
    "        logs = {\"loss\":loss, \"accuracy\":acc}\n",
    "        return {\"loss\":loss, \"accuracy\":acc, \"log\":logs}\n",
    "    \n",
    "    def val_dataloader(self):\n",
    "        dataset = torchvision.datasets.MNIST(root=os.getcwd(), train=False, download=True, transform=self.args[\"transforms\"])\n",
    "        val_loader = DataLoader(dataset=dataset, batch_size=self.args[\"batch_size\"], shuffle=False, num_workers=self.args[\"num_workers\"])\n",
    "        return val_loader\n",
    "    \n",
    "        \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        outputs = self(x)\n",
    "        loss = self.criterion(outputs, y)\n",
    "        acc = self.accuracy(outputs.argmax(dim=1), y)\n",
    "        return {\"val_loss\":loss, \"val_accuracy\":acc}\n",
    "    \n",
    "    def validation_end(self, outputs):\n",
    "        avg_loss = torch.stack([x['val_loss'] for x in outputs]).mean()\n",
    "        avg_acc = torch.stack([x['val_accuracy'] for x in outputs]).mean()\n",
    "        logs = {'val_loss': avg_loss, 'val_log':avg_acc}\n",
    "        return  {'val_loss': avg_loss, 'val_log':avg_acc, 'log': logs}\n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [],
   "source": [
    "model =  LightningViT(args=args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [],
   "source": [
    "# callbacks\n",
    "early_stopping = pl.callbacks.EarlyStopping(\n",
    "    monitor=\"val_acc\",\n",
    "    min_delta=0.05,\n",
    ")\n",
    "\n",
    "model_checkpoint = pl.callbacks.ModelCheckpoint(\n",
    "    filepath=\"model.pth\",\n",
    "    monitor=\"val_acc\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [
    {
     "ename": "MisconfigurationException",
     "evalue": "\n                You requested GPUs: [0]\n                But your machine only has: []\n            ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMisconfigurationException\u001b[0m                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-293-a9e385b06d61>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mprecision\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mdeterministic\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mshow_progress_bar\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m )\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, logger, checkpoint_callback, early_stop_callback, callbacks, default_root_dir, gradient_clip_val, process_position, num_nodes, num_processes, gpus, auto_select_gpus, tpu_cores, log_gpu_memory, progress_bar_refresh_rate, overfit_batches, track_grad_norm, check_val_every_n_epoch, fast_dev_run, accumulate_grad_batches, max_epochs, min_epochs, max_steps, min_steps, limit_train_batches, limit_val_batches, limit_test_batches, val_check_interval, log_save_interval, row_log_interval, distributed_backend, precision, print_nan_grads, weights_summary, weights_save_path, num_sanity_val_steps, truncated_bptt_steps, resume_from_checkpoint, profiler, benchmark, deterministic, reload_dataloaders_every_epoch, auto_lr_find, replace_sampler_ddp, terminate_on_nan, auto_scale_batch_size, prepare_data_per_node, amp_level, num_tpu_cores, use_amp, show_progress_bar, val_percent_check, test_percent_check, train_percent_check, overfit_pct)\u001b[0m\n\u001b[1;32m    522\u001b[0m                             \u001b[0;34m'Trainer was signaled to stop but required minimum epochs'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m                             \u001b[0;34mf' ({self.min_epochs}) or minimum steps ({self.min_steps}) has'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 524\u001b[0;31m                             \u001b[0;34m' not been met. Training will continue...'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    525\u001b[0m                         )\n\u001b[1;32m    526\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/pytorch_lightning/trainer/distrib_parts.py\u001b[0m in \u001b[0;36m_parse_gpu_ids\u001b[0;34m(gpus)\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/pytorch_lightning/trainer/distrib_parts.py\u001b[0m in \u001b[0;36msanitize_gpu_ids\u001b[0;34m(gpus)\u001b[0m\n",
      "\u001b[0;31mMisconfigurationException\u001b[0m: \n                You requested GPUs: [0]\n                But your machine only has: []\n            "
     ]
    }
   ],
   "source": [
    "trainer = pl.Trainer(\n",
    "    checkpoint_callback=model_checkpoint,\n",
    "    early_stop_callback=early_stopping,\n",
    "    gpus=[0],\n",
    "    max_epochs=100,\n",
    "    precision=16,\n",
    "    deterministic=True,\n",
    "    show_progress_bar=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = pl.Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.6 64-bit ('base': conda)",
   "language": "python",
   "name": "python37664bitbaseconda1b4d65181bfe435290e55078ed6e0090"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
